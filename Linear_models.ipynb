{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e92b16e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Linear Regression\n",
    "will test OLS and SGD using random data sets, same seed for consetenticy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa6d2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters / assumptions\n",
    "n_samples = 2000\n",
    "seed = 42\n",
    "rng = np.random.RandomState(seed)\n",
    "noise_std = 3.0 # Standard deviation of the Gaussian noise\n",
    "\n",
    "# Generate 1D feature X in range [-10, 10]\n",
    "X = rng.uniform(-10.0, 10.0, size=(n_samples, 1))\n",
    "\n",
    "# True linear model parameters\n",
    "true_w = 3.0    # slope\n",
    "true_b = 5.0    # intercept\n",
    "\n",
    "# Targets with Gaussian noise, such that y = Mx + b + epsilon(noise)\n",
    "noise = rng.normal(0.0, noise_std, size=(n_samples, 1))\n",
    "y = true_w * X + true_b + noise\n",
    "y = y.ravel()\n",
    "\n",
    "# Quick visualization of the generated data\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.scatter(X, y, s=8, alpha=0.6)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(f\"Generated data with noise (n={n_samples})\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f201782",
   "metadata": {},
   "source": [
    "Fit the line in OLS & SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# two approaches to linear regression: OLS and SGD\n",
    "\n",
    "# 1. OLS using LinearRegression\n",
    "ols_model = LinearRegression()\n",
    "ols_model.fit(X, y)\n",
    "\n",
    "# 2. SGD using SGDRegressor\n",
    "\n",
    "# SGD is sensitive to the scale of data, so we scale X and y \n",
    "scaler_y = StandardScaler()\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Initialize and train the SGD model\n",
    "# We set max_iter and tolerance for convergence\n",
    "sgd_model = SGDRegressor(\n",
    "    loss='squared_error',\n",
    "    alpha=0.0001,\n",
    "    max_iter=1000,\n",
    "    tol=1e-3,\n",
    "    random_state=42\n",
    ")\n",
    "# Fit the model on scaled data\n",
    "sgd_model.fit(X_scaled, y_scaled)\n",
    "\n",
    "# The SGD coefficients are for the *scaled* data. We need to convert them back\n",
    "# to the original scale for comparison.\n",
    "sgd_w_scaled = sgd_model.coef_[0]\n",
    "sgd_b_scaled = sgd_model.intercept_[0]\n",
    "\n",
    "# Unscale the weights (w_orig = w_scaled * (std_y / std_x))\n",
    "sgd_w_orig = sgd_w_scaled * (scaler_y.scale_ / scaler_X.scale_)\n",
    "# Unscale the intercept (b_orig = b_scaled * std_y + mean_y - w_orig * mean_x)\n",
    "sgd_b_orig = sgd_b_scaled * scaler_y.scale_ + scaler_y.mean_ - sgd_w_orig * scaler_X.mean_\n",
    "\n",
    "# --- Results ---\n",
    "# Prepare predictions for plotting and metrics (using unscaled models for consistency)\n",
    "y_pred_ols = ols_model.predict(X)\n",
    "\n",
    "# For SGD, use the unscaled parameters to calculate predictions on unscaled X\n",
    "y_pred_sgd = sgd_w_orig * X.ravel() + sgd_b_orig\n",
    "\n",
    "print(\"## üìä Model Comparison\")\n",
    "print(f\"| Metric | True Value | OLS (LinearRegression) | SGDRegressor (Unscaled) |\")\n",
    "print(f\"| :--- | :--- | :--- | :--- |\")\n",
    "print(f\"| **Slope (w)** | {true_w:.4f} | {ols_model.coef_[0]:.4f} | {sgd_w_orig[0]:.4f} |\")\n",
    "print(f\"| **Intercept (b)** | {true_b:.4f} | {ols_model.intercept_:.4f} | {sgd_b_orig[0]:.4f} |\")\n",
    "print(f\"| **RMSE** | N/A | {mean_squared_error(y, y_pred_ols, squared=False):.4f} | {mean_squared_error(y, y_pred_sgd, squared=False):.4f} |\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, y, s=8, alpha=0.5, label='Generated Data')\n",
    "plt.plot(X, true_w * X + true_b, color='black', linestyle='--', linewidth=2, label='True Line')\n",
    "plt.plot(X, y_pred_ols, color='red', linestyle='-', linewidth=2, label=f'OLS Fit (w={ols_model.coef_[0]:.2f})')\n",
    "plt.plot(X, y_pred_sgd, color='green', linestyle='-', linewidth=1.5, alpha=0.7, label=f'SGD Fit (w={sgd_w_orig[0]:.2f})')\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Comparison of OLS and SGD Linear Fits\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f90cb",
   "metadata": {},
   "source": [
    "Add 20 outlier data points randomly distributed, set them and their target values far away from the original data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d962a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming rng = np.random.RandomState(42) is defined.\n",
    "# We'll use the same noise standard deviation (3.0) as the inliers for the random component.\n",
    "noise_std = 3.0\n",
    "\n",
    "# Parameters for Outliers\n",
    "n_outliers = 20\n",
    "\n",
    "# 1. Generate Outlier Features (X_outlier)\n",
    "X_outliers = rng.uniform(-15.0, 15.0, size=(n_outliers, 1))\n",
    "\n",
    "# 2. Generate Outlier Targets (y_outlier)\n",
    "# Calculate the \"correct\" y value\n",
    "y_outliers_base = 3.0 * X_outliers + 5.0\n",
    "\n",
    "# Generate the intentional offset (+40 / -30)\n",
    "intentional_offset = rng.choice([40.0, -30.0], size=(n_outliers, 1))\n",
    "\n",
    "# Generate random Gaussian noise (epsilon) for the outliers\n",
    "noise_outlier = rng.normal(0.0, noise_std, size=(n_outliers, 1))\n",
    "\n",
    "# Final outlier target: y = (Mx + b) + Intentional_Offset + Epsilon_Noise\n",
    "y_outliers = (y_outliers_base + intentional_offset + noise_outlier).ravel()\n",
    "\n",
    "# 3. Combine Datasets, concatenating the original and outlier data\n",
    "X_combined = np.vstack((X, X_outliers))\n",
    "y_combined = np.hstack((y, y_outliers))\n",
    "\n",
    "print(f\"Original dataset size: {len(X)}\")\n",
    "print(f\"Combined dataset size: {len(X_combined)} ({len(X)} original + {n_outliers} outliers)\")\n",
    "\n",
    "# Fit OLS on the combined data\n",
    "ols_outlier_model = LinearRegression()\n",
    "ols_outlier_model.fit(X_combined, y_combined)\n",
    "\n",
    "# Original OLS fit parameters (from previous step for comparison)\n",
    "# Assuming ols_model, true_w, true_b are available from the previous step\n",
    "ols_clean_w = ols_model.coef_[0]\n",
    "ols_clean_b = ols_model.intercept_\n",
    "\n",
    "# --- Results ---\n",
    "ols_outlier_w = ols_outlier_model.coef_[0]\n",
    "ols_outlier_b = ols_outlier_model.intercept_\n",
    "\n",
    "y_pred_outlier = ols_outlier_model.predict(X_combined)\n",
    "ols_outlier_rmse = mean_squared_error(y_combined, y_pred_outlier, squared=False)\n",
    "\n",
    "print(\"\\n## ‚ö†Ô∏è Impact of Outliers on OLS\")\n",
    "print(f\"| Metric | True Value | Clean OLS Fit | OLS Fit with {n_outliers} Outliers | Change |\")\n",
    "print(f\"| :--- | :--- | :--- | :--- | :--- |\")\n",
    "print(f\"| **Slope (w)** | {true_w:.4f} | {ols_clean_w:.4f} | **{ols_outlier_w:.4f}** | {(ols_outlier_w - true_w) / true_w * 100:.1f}% |\")\n",
    "print(f\"| **Intercept (b)** | {true_b:.4f} | {ols_clean_b:.4f} | **{ols_outlier_b:.4f}** | {(ols_outlier_b - true_b) / true_b * 100:.1f}% |\")\n",
    "print(f\"| **RMSE** | N/A | {mean_squared_error(y, ols_model.predict(X), squared=False):.4f} | **{ols_outlier_rmse:.4f}** | N/A |\")\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(X, y, s=8, alpha=0.5, label='Original Data (2000 points)')\n",
    "plt.scatter(X_outliers, y_outliers, s=25, color='red', marker='x', label=f'Outliers ({n_outliers} points)')\n",
    "\n",
    "# Plot the clean model (for context)\n",
    "plt.plot(X_combined, ols_clean_w * X_combined + ols_clean_b,\n",
    "         color='green', linestyle='--', linewidth=2, label=f'Clean OLS Fit (w={ols_clean_w:.2f})')\n",
    "\n",
    "# Plot the outlier-contaminated model\n",
    "plt.plot(X_combined, ols_outlier_w * X_combined + ols_outlier_b,\n",
    "         color='blue', linestyle='-', linewidth=2, label=f'Contaminated OLS Fit (w={ols_outlier_w:.2f})')\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Effect of Outliers on Ordinary Least Squares (OLS)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baised outliers using only positive offsets\n",
    "# Assuming rng = np.random.RandomState(42) is defined.\n",
    "# We'll use the same noise standard deviation (3.0) as the inliers for the random component.\n",
    "noise_std = 3.0\n",
    "\n",
    "# Parameters for Outliers\n",
    "n_outliers = 20\n",
    "\n",
    "# 1. Generate Outlier Features (X_outlier)\n",
    "X_outliers = rng.uniform(-15.0, 15.0, size=(n_outliers, 1))\n",
    "\n",
    "# 2. Generate Outlier Targets (y_outlier)\n",
    "# Calculate the \"correct\" y value\n",
    "y_outliers_base = 3.0 * X_outliers + 5.0\n",
    "\n",
    "# Generate the intentional offset (+40 / -30)\n",
    "intentional_offset = rng.choice([40.0, 80], size=(n_outliers, 1))\n",
    "\n",
    "# Generate random Gaussian noise (epsilon) for the outliers\n",
    "noise_outlier = rng.normal(0.0, noise_std, size=(n_outliers, 1))\n",
    "\n",
    "# Final outlier target: y = (Mx + b) + Intentional_Offset + Epsilon_Noise\n",
    "y_outliers = (y_outliers_base + intentional_offset + noise_outlier).ravel()\n",
    "\n",
    "# 3. Combine Datasets, concatenating the original and outlier data\n",
    "X_combined = np.vstack((X, X_outliers))\n",
    "y_combined = np.hstack((y, y_outliers))\n",
    "\n",
    "print(f\"Original dataset size: {len(X)}\")\n",
    "print(f\"Combined dataset size: {len(X_combined)} ({len(X)} original + {n_outliers} outliers)\")\n",
    "\n",
    "# Fit OLS on the combined data\n",
    "ols_outlier_model = LinearRegression()\n",
    "ols_outlier_model.fit(X_combined, y_combined)\n",
    "\n",
    "# Original OLS fit parameters (from previous step for comparison)\n",
    "# Assuming ols_model, true_w, true_b are available from the previous step\n",
    "ols_clean_w = ols_model.coef_[0]\n",
    "ols_clean_b = ols_model.intercept_\n",
    "\n",
    "# --- Results ---\n",
    "ols_outlier_w = ols_outlier_model.coef_[0]\n",
    "ols_outlier_b = ols_outlier_model.intercept_\n",
    "\n",
    "y_pred_outlier = ols_outlier_model.predict(X_combined)\n",
    "ols_outlier_rmse = mean_squared_error(y_combined, y_pred_outlier, squared=False)\n",
    "\n",
    "print(\"\\n## ‚ö†Ô∏è Impact of Outliers on OLS\")\n",
    "print(f\"| Metric | True Value | Clean OLS Fit | OLS Fit with {n_outliers} Outliers | Change |\")\n",
    "print(f\"| :--- | :--- | :--- | :--- | :--- |\")\n",
    "print(f\"| **Slope (w)** | {true_w:.4f} | {ols_clean_w:.4f} | **{ols_outlier_w:.4f}** | {(ols_outlier_w - true_w) / true_w * 100:.1f}% |\")\n",
    "print(f\"| **Intercept (b)** | {true_b:.4f} | {ols_clean_b:.4f} | **{ols_outlier_b:.4f}** | {(ols_outlier_b - true_b) / true_b * 100:.1f}% |\")\n",
    "print(f\"| **RMSE** | N/A | {mean_squared_error(y, ols_model.predict(X), squared=False):.4f} | **{ols_outlier_rmse:.4f}** | N/A |\")\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(X, y, s=8, alpha=0.5, label='Original Data (2000 points)')\n",
    "plt.scatter(X_outliers, y_outliers, s=25, color='red', marker='x', label=f'Outliers ({n_outliers} points)')\n",
    "\n",
    "# Plot the clean model (for context)\n",
    "plt.plot(X_combined, ols_clean_w * X_combined + ols_clean_b,\n",
    "         color='green', linestyle='--', linewidth=2, label=f'Clean OLS Fit (w={ols_clean_w:.2f})')\n",
    "\n",
    "# Plot the outlier-contaminated model\n",
    "plt.plot(X_combined, ols_outlier_w * X_combined + ols_outlier_b,\n",
    "         color='blue', linestyle='-', linewidth=2, label=f'Contaminated OLS Fit (w={ols_outlier_w:.2f})')\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Effect of Outliers on Ordinary Least Squares (OLS)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79987fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baised outliers using only positive offsets\n",
    "# Assuming rng = np.random.RandomState(42) is defined.\n",
    "# We'll use the same noise standard deviation (3.0) as the inliers for the random component.\n",
    "noise_std = 3.0\n",
    "\n",
    "# Parameters for Outliers\n",
    "n_outliers = 20\n",
    "\n",
    "# 1. Generate Outlier Features (X_outlier)\n",
    "X_outliers = rng.uniform(-15.0, 15.0, size=(n_outliers, 1))\n",
    "\n",
    "# 2. Generate Outlier Targets (y_outlier)\n",
    "# Calculate the \"correct\" y value\n",
    "y_outliers_base = 3.0 * X_outliers + 5.0\n",
    "\n",
    "# Generate the intentional offset (+40 / -30)\n",
    "intentional_offset = rng.choice([40.0, 80], size=(n_outliers, 1))\n",
    "\n",
    "# Generate random Gaussian noise (epsilon) for the outliers\n",
    "noise_outlier = rng.normal(0.0, noise_std, size=(n_outliers, 1))\n",
    "\n",
    "# Final outlier target: y = (Mx + b) + Intentional_Offset + Epsilon_Noise\n",
    "y_outliers = (y_outliers_base + intentional_offset + noise_outlier).ravel()\n",
    "\n",
    "# 3. Combine Datasets, concatenating the original and outlier data\n",
    "X_combined = np.vstack((X, X_outliers))\n",
    "y_combined = np.hstack((y, y_outliers))\n",
    "\n",
    "print(f\"Original dataset size: {len(X)}\")\n",
    "print(f\"Combined dataset size: {len(X_combined)} ({len(X)} original + {n_outliers} outliers)\")\n",
    "\n",
    "# Fit OLS on the combined data\n",
    "ols_outlier_model = LinearRegression()\n",
    "ols_outlier_model.fit(X_combined, y_combined)\n",
    "\n",
    "# Original OLS fit parameters (from previous step for comparison)\n",
    "# Assuming ols_model, true_w, true_b are available from the previous step\n",
    "ols_clean_w = ols_model.coef_[0]\n",
    "ols_clean_b = ols_model.intercept_\n",
    "\n",
    "# --- Results ---\n",
    "ols_outlier_w = ols_outlier_model.coef_[0]\n",
    "ols_outlier_b = ols_outlier_model.intercept_\n",
    "\n",
    "y_pred_outlier = ols_outlier_model.predict(X_combined)\n",
    "ols_outlier_rmse = mean_squared_error(y_combined, y_pred_outlier, squared=False)\n",
    "\n",
    "print(\"\\n## ‚ö†Ô∏è Impact of Outliers on OLS\")\n",
    "print(f\"| Metric | True Value | Clean OLS Fit | OLS Fit with {n_outliers} Outliers | Change |\")\n",
    "print(f\"| :--- | :--- | :--- | :--- | :--- |\")\n",
    "print(f\"| **Slope (w)** | {true_w:.4f} | {ols_clean_w:.4f} | **{ols_outlier_w:.4f}** | {(ols_outlier_w - true_w) / true_w * 100:.1f}% |\")\n",
    "print(f\"| **Intercept (b)** | {true_b:.4f} | {ols_clean_b:.4f} | **{ols_outlier_b:.4f}** | {(ols_outlier_b - true_b) / true_b * 100:.1f}% |\")\n",
    "print(f\"| **RMSE** | N/A | {mean_squared_error(y, ols_model.predict(X), squared=False):.4f} | **{ols_outlier_rmse:.4f}** | N/A |\")\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(X, y, s=8, alpha=0.5, label='Original Data (2000 points)')\n",
    "plt.scatter(X_outliers, y_outliers, s=25, color='red', marker='x', label=f'Outliers ({n_outliers} points)')\n",
    "\n",
    "# Plot the clean model (for context)\n",
    "plt.plot(X_combined, ols_clean_w * X_combined + ols_clean_b,\n",
    "         color='green', linestyle='--', linewidth=2, label=f'Clean OLS Fit (w={ols_clean_w:.2f})')\n",
    "\n",
    "# Plot the outlier-contaminated model\n",
    "plt.plot(X_combined, ols_outlier_w * X_combined + ols_outlier_b,\n",
    "         color='blue', linestyle='-', linewidth=2, label=f'Contaminated OLS Fit (w={ols_outlier_w:.2f})')\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Effect of Outliers on Ordinary Least Squares (OLS)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78774617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standalone script for Outlier Damage Threshold Analysis (we should test only OLS here, don't run this cell in the notebook, only for testing :D )\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 1. Data Setup (Fixed Inliers)\n",
    "n_inliers = 2000\n",
    "true_w = 3.0\n",
    "true_b = 5.0\n",
    "noise_std = 3.0\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Inlier data (fixed)\n",
    "X_inliers = rng.uniform(-10.0, 10.0, size=(n_inliers, 1))\n",
    "noise_inliers = rng.normal(0.0, noise_std, size=(n_inliers, 1))\n",
    "y_inliers = true_w * X_inliers + true_b + noise_inliers\n",
    "y_inliers = y_inliers.ravel()\n",
    "\n",
    "# 2. Simulation Parameters\n",
    "outlier_counts = [20, 50, 100, 150, 200, 300, 400, 500, 1000,1500,2000]\n",
    "results = []\n",
    "target_w_deviation = 0.1 # 10% deviation in slope (w = 3.3)\n",
    "\n",
    "# 3. Simulation Loop (OLS Only)\n",
    "for N_outlier in outlier_counts:\n",
    "    # 3a. Generate Outliers\n",
    "    X_outliers = rng.uniform(-15.0, 15.0, size=(N_outlier, 1))\n",
    "    y_outliers_base = 3.0 * X_outliers + 5.0\n",
    "    # Add a large, biased offset\n",
    "    y_outliers = y_outliers_base.ravel() + rng.choice([300.0, -300.0], size=N_outlier)\n",
    "\n",
    "    # 3b. Combine Data\n",
    "    X_combined = np.vstack((X_inliers, X_outliers))\n",
    "    y_combined = np.hstack((y_inliers, y_outliers))\n",
    "\n",
    "    # 3c. Fit OLS (LinearRegression)\n",
    "    ols_model = LinearRegression()\n",
    "    ols_model.fit(X_combined, y_combined)\n",
    "    ols_w_fit = ols_model.coef_[0]\n",
    "\n",
    "    # 3d. Store results\n",
    "    results.append({\n",
    "        'N_outlier': N_outlier,\n",
    "        'OLS_w': ols_w_fit\n",
    "    })\n",
    "\n",
    "# 4. Plotting Results\n",
    "n_outlier_array = np.array([r['N_outlier'] for r in results])\n",
    "ols_w_array = np.array([r['OLS_w'] for r in results])\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# Plot OLS results\n",
    "plt.plot(n_outlier_array, ols_w_array, marker='o', linestyle='-', color='blue', label='OLS Fitted Slope')\n",
    "\n",
    "# Plot True Value and Deviation Threshold\n",
    "plt.axhline(y=true_w, color='green', linestyle='-', linewidth=2, label=f'True Slope (w={true_w})')\n",
    "plt.axhline(y=true_w * (1 + target_w_deviation), color='red', linestyle=':', label=f'10% Deviation Threshold (w={true_w * (1 + target_w_deviation):.2f})')\n",
    "\n",
    "plt.xlabel(\"Number of Outliers Added\")\n",
    "plt.ylabel(\"Fitted OLS Slope Coefficient (w)\")\n",
    "plt.title(\"OLS Slope Deviation vs. Outlier Count (N_inliers = 2000)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "#5. Summary Table\n",
    "print(\"\\n## üéØ OLS Threshold Analysis Summary\")\n",
    "print(\"| N_Outlier | OLS Slope (w) | % Change from True |\")\n",
    "print(\"| :--- | :--- | :--- |\")\n",
    "for r in results:\n",
    "    ols_w_change_percent = (r['OLS_w'] - true_w) / true_w * 100\n",
    "    print(f\"| {r['N_outlier']:<9} | {r['OLS_w']:.4f} | {ols_w_change_percent:.2f}% |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68414844",
   "metadata": {},
   "source": [
    "Fitting model with RANSAC alogrithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bc809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "# Assuming X_combined, y_combined, ols_clean_w, ols_clean_b, \n",
    "# ols_outlier_w, ols_outlier_b are available from previous steps.\n",
    "\n",
    "# --- Fit RANSAC Model ---\n",
    "# We use the contamination level (2 * noise_std = 6.0) as the threshold for an inlier, but we set a bit higher to account for noise.\n",
    "ransac_model = RANSACRegressor(\n",
    "    min_samples=2,\n",
    "    residual_threshold=8.0,\n",
    "    random_state=42\n",
    ")\n",
    "ransac_model.fit(X_combined, y_combined)\n",
    "\n",
    "# Extract RANSAC coefficients\n",
    "ransac_w = ransac_model.estimator_.coef_[0]\n",
    "ransac_b = ransac_model.estimator_.intercept_\n",
    "\n",
    "# Determine inliers/outliers for plotting\n",
    "inlier_mask = ransac_model.inlier_mask_\n",
    "outlier_mask = np.logical_not(inlier_mask)\n",
    "\n",
    "# --- Comparison Results ---\n",
    "print(\"## üìä RANSAC vs. OLS Fit Comparison\")\n",
    "print(f\"| Metric | True Value | Contaminated OLS | RANSAC Robust Fit |\")\n",
    "print(f\"| :--- | :--- | :--- | :--- |\")\n",
    "print(f\"| **Slope (w)** | {true_w:.4f} | {ols_outlier_w:.4f} | **{ransac_w:.4f}** |\")\n",
    "print(f\"| **Intercept (b)** | {true_b:.4f} | {ols_outlier_b:.4f} | **{ransac_b:.4f}** |\")\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the data\n",
    "# plt.scatter(X_combined[inlier_mask], y_combined[inlier_mask], s=10, alpha=0.5, color='gray', label='RANSAC Inliers')\n",
    "# plt.scatter(X_combined[outlier_mask], y_combined[outlier_mask], s=40, color='red', marker='x', label='RANSAC Outliers Ignored')\n",
    "\n",
    "# Define x-range for plotting lines\n",
    "X_plot = np.linspace(X_combined.min(), X_combined.max(), 100).reshape(-1, 1)\n",
    "\n",
    "# Plot the Contaminated OLS Fit (Sensitive Model)\n",
    "y_ols_contam = ols_outlier_w * X_plot.ravel() + ols_outlier_b\n",
    "plt.plot(X_plot, y_ols_contam, color='blue', linestyle='-', linewidth=2, label=f'OLS Fit (Sensitive, w={ols_outlier_w:.2f})')\n",
    "\n",
    "# Plot the RANSAC Fit (Robust Model)\n",
    "y_ransac = ransac_w * X_plot.ravel() + ransac_b\n",
    "plt.plot(X_plot, y_ransac, color='orange', linestyle='-', linewidth=3, label=f'RANSAC Fit (Robust, w={ransac_w:.2f})')\n",
    "\n",
    "# Plot the ideal clean fit for reference\n",
    "y_clean = ols_clean_w * X_plot.ravel() + ols_clean_b\n",
    "plt.plot(X_plot, y_clean, color='green', linestyle='--', linewidth=2, label=f'Clean OLS Fit (Ideal, w={ols_clean_w:.2f})')\n",
    "\n",
    "plt.ylim(np.min(y_combined) - 50, np.max(y_combined) + 50)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"RANSAC vs. OLS: Robustness to Outliers\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
